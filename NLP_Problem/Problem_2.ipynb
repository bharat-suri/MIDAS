{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDAS Summer Internship Task\n",
    "### Problem 2: NLP Problem\n",
    "**Suggestion Mining**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import FastText\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Creating word embeddings***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "x_train = []\n",
    "sentences = []\n",
    "with open('data/V1.4_Training.csv', 'r') as training_file:\n",
    "    for line in csv.reader(training_file, delimiter=','):\n",
    "        y_train.append(int(line[2]))\n",
    "        sentences.append(line[1].strip('\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2085"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x == 1, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6415"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x == 0, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sentences:\n",
    "    temp = []\n",
    "    \n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "    \n",
    "    x_train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : resetting layer weights\n",
      "INFO : Total number of ngrams is 0\n",
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO : collected 11213 word types from a corpus of 170850 raw words and 8500 sentences\n",
      "INFO : Loading a fresh vocabulary\n",
      "INFO : effective_min_count=1 retains 11213 unique words (100% of original 11213, drops 0)\n",
      "INFO : effective_min_count=1 leaves 170850 word corpus (100% of original 170850, drops 0)\n",
      "INFO : deleting the raw counts dictionary of 11213 items\n",
      "INFO : sample=0.001 downsamples 49 most-common words\n",
      "INFO : downsampling leaves estimated 125858 word corpus (73.7% of prior 170850)\n",
      "INFO : estimated required memory for 11213 words, 111769 buckets and 100 dimensions: 63448060 bytes\n",
      "INFO : resetting layer weights\n",
      "INFO : Total number of ngrams is 111769\n",
      "INFO : training model with 3 workers on 11213 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : EPOCH 1 - PROGRESS: at 94.55% examples, 107448 words/s, in_qsize 1, out_qsize 1\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 1 : training on 170850 raw words (126268 effective words) took 1.1s, 113570 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : EPOCH 2 - PROGRESS: at 93.78% examples, 117657 words/s, in_qsize 1, out_qsize 1\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 2 : training on 170850 raw words (125957 effective words) took 1.0s, 122295 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : EPOCH 3 - PROGRESS: at 93.78% examples, 115413 words/s, in_qsize 1, out_qsize 1\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 3 : training on 170850 raw words (125712 effective words) took 1.0s, 121566 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : EPOCH 4 - PROGRESS: at 100.00% examples, 125686 words/s, in_qsize 0, out_qsize 1\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 4 : training on 170850 raw words (125951 effective words) took 1.0s, 125345 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 5 : training on 170850 raw words (125780 effective words) took 0.9s, 141684 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 6 : training on 170850 raw words (125822 effective words) took 0.9s, 141932 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 7 : training on 170850 raw words (125901 effective words) took 0.9s, 143361 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 8 : training on 170850 raw words (125625 effective words) took 0.9s, 146470 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 9 : training on 170850 raw words (125577 effective words) took 1.0s, 127088 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 10 : training on 170850 raw words (126067 effective words) took 0.9s, 144808 effective words/s\n",
      "INFO : training on a 1708500 raw words (1258660 effective words) took 9.7s, 129830 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# w2v = Word2Vec(x_train, sg=1, size=100, min_count=1, window=10, workers=2, iter=10)\n",
    "ft = FastText(x_train, size=100, window=10, min_count=1, iter=10)\n",
    "# os.mkdir('embedding')\n",
    "# w2v.wv.save_word2vec_format('embedding/w2v')\n",
    "# ft.save('embedding/ft')\n",
    "# wv = w2v.wv\n",
    "# del w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Using Linear SVC***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8500, 100)\n",
      "(8500,)\n"
     ]
    }
   ],
   "source": [
    "x_train_v = np.array([np.array(list(map(lambda x: ft.wv[x], x))) for x in x_train])\n",
    "x_train_v = np.array([np.sum(i, axis=0) for i in x_train_v])\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(x_train_v.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.where(y_train == 1)\n",
    "over_sampled_x = np.vstack((x_train_v, x_train_v[index], x_train_v[index]))\n",
    "over_sampled_y = np.hstack((y_train, y_train[index], y_train[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(over_sampled_x, over_sampled_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = svm.SVC()\n",
    "# svc.fit(over_sampled_x, over_sampled_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "x_test = []\n",
    "sentences = []\n",
    "with open('data/SubtaskA_Trial_Test_Labeled.csv', 'r', encoding='ISO-8859-1') as testing_file:\n",
    "    next(testing_file)\n",
    "    for line in csv.reader(testing_file, delimiter=','):\n",
    "        y_test.append(int(line[2]))\n",
    "        sentences.append(line[1].strip('\"'))\n",
    "        \n",
    "for i in sentences:\n",
    "    temp = []\n",
    "    \n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "    \n",
    "    x_test.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : resetting layer weights\n",
      "INFO : Total number of ngrams is 0\n",
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO : collected 2279 word types from a corpus of 12090 raw words and 592 sentences\n",
      "INFO : Loading a fresh vocabulary\n",
      "INFO : effective_min_count=1 retains 2279 unique words (100% of original 2279, drops 0)\n",
      "INFO : effective_min_count=1 leaves 12090 word corpus (100% of original 12090, drops 0)\n",
      "INFO : deleting the raw counts dictionary of 2279 items\n",
      "INFO : sample=0.001 downsamples 48 most-common words\n",
      "INFO : downsampling leaves estimated 8680 word corpus (71.8% of prior 12090)\n",
      "INFO : estimated required memory for 2279 words, 21962 buckets and 100 dimensions: 12234916 bytes\n",
      "INFO : resetting layer weights\n",
      "INFO : Total number of ngrams is 21962\n",
      "INFO : training model with 3 workers on 2279 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 1 : training on 12090 raw words (8759 effective words) took 0.1s, 90546 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 2 : training on 12090 raw words (8680 effective words) took 0.1s, 89304 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 3 : training on 12090 raw words (8687 effective words) took 0.1s, 91301 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 4 : training on 12090 raw words (8679 effective words) took 0.1s, 94534 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 5 : training on 12090 raw words (8716 effective words) took 0.1s, 97302 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 6 : training on 12090 raw words (8713 effective words) took 0.1s, 89181 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 7 : training on 12090 raw words (8743 effective words) took 0.1s, 105701 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 8 : training on 12090 raw words (8706 effective words) took 0.1s, 85472 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 9 : training on 12090 raw words (8641 effective words) took 0.1s, 89498 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 10 : training on 12090 raw words (8690 effective words) took 0.1s, 87104 effective words/s\n",
      "INFO : training on a 120900 raw words (87014 effective words) took 1.0s, 85995 effective words/s\n",
      "WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "ft = FastText(x_test, size=100, window=10, min_count=1, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_v = np.array([np.array(list(map(lambda x: ft.wv[x], x))) for x in x_test])\n",
    "x_test_v = np.array([np.sum(i, axis=0) for i in x_test_v])\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lab = gnb.predict(x_test_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : resetting layer weights\n",
      "INFO : Total number of ngrams is 0\n",
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO : collected 2857 word types from a corpus of 15072 raw words and 833 sentences\n",
      "INFO : Loading a fresh vocabulary\n",
      "INFO : effective_min_count=1 retains 2857 unique words (100% of original 2857, drops 0)\n",
      "INFO : effective_min_count=1 leaves 15072 word corpus (100% of original 15072, drops 0)\n",
      "INFO : deleting the raw counts dictionary of 2857 items\n",
      "INFO : sample=0.001 downsamples 54 most-common words\n",
      "INFO : downsampling leaves estimated 11001 word corpus (73.0% of prior 15072)\n",
      "INFO : estimated required memory for 2857 words, 28465 buckets and 100 dimensions: 15759116 bytes\n",
      "INFO : resetting layer weights\n",
      "INFO : Total number of ngrams is 28465\n",
      "INFO : training model with 3 workers on 2857 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 1 : training on 15072 raw words (11023 effective words) took 0.1s, 94052 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 2 : training on 15072 raw words (11015 effective words) took 0.1s, 80912 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 3 : training on 15072 raw words (11042 effective words) took 0.1s, 99226 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 4 : training on 15072 raw words (11019 effective words) took 0.1s, 100340 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 5 : training on 15072 raw words (10992 effective words) took 0.1s, 92153 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 6 : training on 15072 raw words (10945 effective words) took 0.1s, 104929 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 7 : training on 15072 raw words (11053 effective words) took 0.1s, 99513 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 8 : training on 15072 raw words (10952 effective words) took 0.1s, 105049 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 9 : training on 15072 raw words (10979 effective words) took 0.1s, 97681 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 10 : training on 15072 raw words (10944 effective words) took 0.1s, 102972 effective words/s\n",
      "INFO : training on a 150720 raw words (109964 effective words) took 1.2s, 92875 effective words/s\n",
      "WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "y_eval = []\n",
    "x_eval = []\n",
    "id_eval = []\n",
    "sentences = []\n",
    "with open('data/SubtaskA_EvaluationData.csv', 'r', encoding='utf-8') as eval_file:\n",
    "    for line in csv.reader(eval_file, delimiter=','):\n",
    "        id_eval.append(line[0])\n",
    "        sentences.append(line[1].strip('\"'))\n",
    "        \n",
    "for i in sentences:\n",
    "    temp = []\n",
    "    \n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "    \n",
    "    x_eval.append(temp)\n",
    "\n",
    "ft = FastText(x_eval, size=100, window=10, min_count=1, iter=10)\n",
    "x_eval_v = np.array([np.array(list(map(lambda x: ft.wv[x], x))) for x in x_eval])\n",
    "x_eval_v = np.array([np.sum(i, axis=0) for i in x_eval_v])\n",
    "y_pred = gnb.predict(x_eval_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ = pd.DataFrame(data=sentences, index=id_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_['pred'] = pd.Series(y_pred, index=eval_.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_.to_csv('bharat_suri.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Training the LSTM___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([np.array(list(map(lambda x: wv.get_vector(x), x))) for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [np.sum(i, axis=0) for i in inputs]\n",
    "inputs = torch.Tensor(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.Tensor(labels).view(-1, 1)\n",
    "print(inputs.size())\n",
    "print(labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_, hidden):\n",
    "        combined = torch.cat([input_, hidden], 0)\n",
    "        a1 = self.fc1(combined)\n",
    "        a2 = self.relu(a1)\n",
    "        a3 = self.fc2(a2)\n",
    "        output = self.sigmoid(a3)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "i_size = 100\n",
    "h_size = 100\n",
    "o_size = 1\n",
    "\n",
    "model = Model(i_size, h_size, o_size)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in range(0, len(inputs) + 1, 200):\n",
    "    hidden = model.init_hidden()\n",
    "    output = model(inputs[0:batch], hidden)\n",
    "    loss = criterion(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCELoss()\n",
    "# opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# def train(label, inp):\n",
    "#     hidden = model.init_hidden()\n",
    "    \n",
    "#     opt.zero_grad()\n",
    "    \n",
    "#     for i in inp:\n",
    "#         output, hidden = model(i.view(1, -1), hidden)\n",
    "    \n",
    "#     loss = criterion(output, label)\n",
    "#     loss.backward()\n",
    "#     opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# import math\n",
    "\n",
    "# n_iter = 10000\n",
    "# print_every = 500\n",
    "# plot_every = 100\n",
    "\n",
    "# current_loss = 0\n",
    "# all_losses = []\n",
    "\n",
    "# def time_since(since):\n",
    "#     now = time.time()\n",
    "#     s = now - since\n",
    "#     m = math.floor(s / 60)\n",
    "#     s -= m * 60\n",
    "#     return '%dm %ds' % (m, s)\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# x_train, y_train = inputs[0], labels[0]\n",
    "\n",
    "# for x in range(0, n_iter):\n",
    "#     output, loss = train(y_train, x_train)\n",
    "#     current_loss += loss\n",
    "    \n",
    "#     if x % print_every == 0:\n",
    "#         print('%d %d%% (%s) %.4f' % (x, x / n_iter * 100, time_since(start), loss))\n",
    "        \n",
    "#     if x % plot_every == 0:\n",
    "#         all_losses.append(current_loss / plot_every)\n",
    "#         current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.ticker as ticker\n",
    "# %matplotlib inline\n",
    "\n",
    "# plt.plot(all_losses)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = inputs[0]\n",
    "y_train = labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = model.init_hidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(50):\n",
    "    for i in inputs[:10]:\n",
    "        for j in i:\n",
    "            output, hidden = model(j.view(1, -1), hidden)\n",
    "\n",
    "    loss = criterion(output.squeeze(1), y_train)\n",
    "    print(loss.item())\n",
    "    loss.backward(retain_graph=True)\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i, y in zip(inputs, labels):\n",
    "    for x in i:\n",
    "        output, hidden = model(x.view(1, -1), hidden)\n",
    "    if y == 1 and output > 0.82:\n",
    "        count += 1\n",
    "    elif y == 0 and output <= 0.82:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
