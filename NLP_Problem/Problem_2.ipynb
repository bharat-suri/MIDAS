{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDAS Summer Internship Task\n",
    "### Problem 2: NLP Problem\n",
    "**Suggestion Mining**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import FastText\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Creating word embeddings***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "x_train = []\n",
    "sentences = []\n",
    "with open('data/V1.4_Training.csv', 'r') as training_file:\n",
    "    for line in csv.reader(training_file, delimiter=','):\n",
    "        y_train.append(int(line[2]))\n",
    "        sentences.append(line[1].strip('\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2085"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x == 1, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6415"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x == 0, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sentences:\n",
    "    temp = []\n",
    "    \n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "    \n",
    "    x_train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loading projection weights from embedding/w2v\n",
      "INFO : loaded (11220, 100) matrix from embedding/w2v\n",
      "INFO : loading FastText object from embedding/ft\n",
      "INFO : loading wv recursively from embedding/ft.wv.* with mmap=None\n",
      "INFO : loading vectors_ngrams from embedding/ft.wv.vectors_ngrams.npy with mmap=None\n",
      "INFO : setting ignored attribute vectors_norm to None\n",
      "INFO : setting ignored attribute vectors_vocab_norm to None\n",
      "INFO : setting ignored attribute vectors_ngrams_norm to None\n",
      "INFO : setting ignored attribute buckets_word to None\n",
      "INFO : loading vocabulary recursively from embedding/ft.vocabulary.* with mmap=None\n",
      "INFO : loading trainables recursively from embedding/ft.trainables.* with mmap=None\n",
      "INFO : loading vectors_ngrams_lockf from embedding/ft.trainables.vectors_ngrams_lockf.npy with mmap=None\n",
      "INFO : loaded embedding/ft\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir('embedding'):\n",
    "    w2v = Word2Vec(x_train, sg=1, size=100, min_count=1, window=10, workers=2, iter=10)\n",
    "    ft = FastText(x_train, size=100, window=10, min_count=1, iter=10)\n",
    "    os.mkdir('embedding')\n",
    "    w2v.wv.save_word2vec_format('embedding/w2v')\n",
    "    ft.save('embedding/ft')\n",
    "    wv = w2v.wv\n",
    "    del w2v\n",
    "\n",
    "else:\n",
    "    wv = KeyedVectors.load_word2vec_format('embedding/w2v')\n",
    "    ft = FastText.load('embedding/ft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Using Linear SVC***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8500, 100)\n",
      "(8500,)\n"
     ]
    }
   ],
   "source": [
    "x_train_v = np.array([np.array(list(map(lambda x: wv[x], x))) for x in x_train])\n",
    "x_train_v = np.array([np.sum(i, axis=0) for i in x_train_v])\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(x_train_v.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='linear')\n",
    "svc.fit(x_train_v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "x_test = []\n",
    "sentences = []\n",
    "# test = pd.read_csv('data/SubtaskA_Trial_Test_Labeled.csv', encoding='latin-1')\n",
    "with open('data/SubtaskA_Trial_Test_Labeled.csv', 'r', encoding='ISO-8859-1') as testing_file:\n",
    "    next(testing_file)\n",
    "    for line in csv.reader(testing_file, delimiter=','):\n",
    "        y_test.append(int(line[2]))\n",
    "        sentences.append(line[1].strip('\"'))\n",
    "        \n",
    "for i in sentences:\n",
    "    temp = []\n",
    "    \n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "    \n",
    "    x_test.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO : collected 2279 word types from a corpus of 12090 raw words and 592 sentences\n",
      "INFO : Loading a fresh vocabulary\n",
      "INFO : min_count=1 retains 2279 unique words (100% of original 2279, drops 0)\n",
      "INFO : min_count=1 leaves 12090 word corpus (100% of original 12090, drops 0)\n",
      "INFO : deleting the raw counts dictionary of 2279 items\n",
      "INFO : sample=0.001 downsamples 48 most-common words\n",
      "INFO : downsampling leaves estimated 8680 word corpus (71.8% of prior 12090)\n",
      "INFO : estimated required memory for 2279 words, 21962 buckets and 100 dimensions: 12234916 bytes\n",
      "INFO : resetting layer weights\n",
      "INFO : Total number of ngrams is 21962\n",
      "INFO : training model with 3 workers on 2279 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 1 : training on 12090 raw words (8667 effective words) took 0.1s, 65202 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 2 : training on 12090 raw words (8635 effective words) took 0.2s, 56380 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 3 : training on 12090 raw words (8665 effective words) took 0.1s, 72484 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 4 : training on 12090 raw words (8691 effective words) took 0.1s, 100670 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 5 : training on 12090 raw words (8621 effective words) took 0.1s, 68076 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 6 : training on 12090 raw words (8674 effective words) took 0.1s, 61117 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 7 : training on 12090 raw words (8646 effective words) took 0.2s, 48007 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 8 : training on 12090 raw words (8651 effective words) took 0.1s, 81977 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 9 : training on 12090 raw words (8662 effective words) took 0.1s, 73238 effective words/s\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : EPOCH - 10 : training on 12090 raw words (8672 effective words) took 0.1s, 99790 effective words/s\n",
      "INFO : training on a 120900 raw words (86584 effective words) took 1.5s, 58174 effective words/s\n",
      "WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "ft = FastText(x_test, size=100, window=10, min_count=1, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_v = np.array([np.array(list(map(lambda x: ft.wv[x], x))) for x in x_test])\n",
    "x_test_v = np.array([np.sum(i, axis=0) for i in x_test_v])\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(svc.score(x_test_v, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Training the LSTM___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([np.array(list(map(lambda x: wv.get_vector(x), x))) for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [np.sum(i, axis=0) for i in inputs]\n",
    "inputs = torch.Tensor(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.Tensor(labels).view(-1, 1)\n",
    "print(inputs.size())\n",
    "print(labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_, hidden):\n",
    "        combined = torch.cat([input_, hidden], 0)\n",
    "        a1 = self.fc1(combined)\n",
    "        a2 = self.relu(a1)\n",
    "        a3 = self.fc2(a2)\n",
    "        output = self.sigmoid(a3)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "i_size = 100\n",
    "h_size = 100\n",
    "o_size = 1\n",
    "\n",
    "model = Model(i_size, h_size, o_size)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in range(0, len(inputs) + 1, 200):\n",
    "    hidden = model.init_hidden()\n",
    "    output = model(inputs[0:batch], hidden)\n",
    "    loss = criterion(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCELoss()\n",
    "# opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# def train(label, inp):\n",
    "#     hidden = model.init_hidden()\n",
    "    \n",
    "#     opt.zero_grad()\n",
    "    \n",
    "#     for i in inp:\n",
    "#         output, hidden = model(i.view(1, -1), hidden)\n",
    "    \n",
    "#     loss = criterion(output, label)\n",
    "#     loss.backward()\n",
    "#     opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# import math\n",
    "\n",
    "# n_iter = 10000\n",
    "# print_every = 500\n",
    "# plot_every = 100\n",
    "\n",
    "# current_loss = 0\n",
    "# all_losses = []\n",
    "\n",
    "# def time_since(since):\n",
    "#     now = time.time()\n",
    "#     s = now - since\n",
    "#     m = math.floor(s / 60)\n",
    "#     s -= m * 60\n",
    "#     return '%dm %ds' % (m, s)\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# x_train, y_train = inputs[0], labels[0]\n",
    "\n",
    "# for x in range(0, n_iter):\n",
    "#     output, loss = train(y_train, x_train)\n",
    "#     current_loss += loss\n",
    "    \n",
    "#     if x % print_every == 0:\n",
    "#         print('%d %d%% (%s) %.4f' % (x, x / n_iter * 100, time_since(start), loss))\n",
    "        \n",
    "#     if x % plot_every == 0:\n",
    "#         all_losses.append(current_loss / plot_every)\n",
    "#         current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.ticker as ticker\n",
    "# %matplotlib inline\n",
    "\n",
    "# plt.plot(all_losses)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = inputs[0]\n",
    "y_train = labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = model.init_hidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(50):\n",
    "    for i in inputs[:10]:\n",
    "        for j in i:\n",
    "            output, hidden = model(j.view(1, -1), hidden)\n",
    "\n",
    "    loss = criterion(output.squeeze(1), y_train)\n",
    "    print(loss.item())\n",
    "    loss.backward(retain_graph=True)\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i, y in zip(inputs, labels):\n",
    "    for x in i:\n",
    "        output, hidden = model(x.view(1, -1), hidden)\n",
    "    if y == 1 and output > 0.82:\n",
    "        count += 1\n",
    "    elif y == 0 and output <= 0.82:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
